{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based on the [DeepLift tutorial](https://github.com/kundajelab/deeplift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model trained on GRIDSS results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify the data directory\n",
    "data_dir = \"\"\n",
    "model = load_model(os.path.join(data_dir,'gridss_model.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = model.predict_proba(X, batch_size=1000, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the entropy of the model predictions (see the [Integrated-Gradients howto, Sanity Checking Baselines](https://github.com/ankurtaly/Integrated-Gradients/blob/master/howto.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropy(b):\n",
    "    return np.apply_along_axis(lambda p: -(np.sum(p*np.log(p))), 1, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_entropy = get_entropy(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install matplotlib -y\n",
    "# if matplotlib does not work, you might need to install nomkl\n",
    "# !conda install nomkl -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n, bins, patches = plt.hist(probs_entropy, 100, density=False, facecolor='g', alpha=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "print (\"keras version\",keras.__version__)\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Convolution1D, Lambda, \\\n",
    "    Convolution2D, Flatten, \\\n",
    "    Reshape, LSTM, Dropout, TimeDistributed, BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "#create a keras 2 model with the same architecture\n",
    "#set the weights for each layer using the hdf5\n",
    "#weights file\n",
    "\n",
    "class_number = 3\n",
    "dim_length = 200\n",
    "dim_channels = 46\n",
    "layers = 4 # 2\n",
    "filters = [32] * layers  # 4\n",
    "fc_hidden_nodes = 8\n",
    "learning_rate = 10 ** (-4)\n",
    "regularization_rate = 10 ** (-1)\n",
    "kernel_size = 7\n",
    "drp_out1 = 0\n",
    "drp_out2 = 0\n",
    "\n",
    "outputdim = class_number  # number of classes\n",
    "\n",
    "weightinit = 'lecun_uniform'  # weight initialization\n",
    "\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    BatchNormalization(\n",
    "        input_shape=(\n",
    "            dim_length,\n",
    "            dim_channels)))\n",
    "\n",
    "for filter_number in filters:\n",
    "    model.add(Convolution1D(filter_number, kernel_size=kernel_size, padding='same',\n",
    "                            kernel_regularizer=l2(regularization_rate),\n",
    "                            kernel_initializer=weightinit))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "# model.add(Dropout(drp_out1))\n",
    "model.add(Dense(units=fc_hidden_nodes,\n",
    "                kernel_regularizer=l2(regularization_rate),\n",
    "                kernel_initializer=weightinit))  # Fully connected layer\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))  # Relu activation\n",
    "# model.add(Dropout(drp_out2))\n",
    "model.add(Dense(units=outputdim, kernel_initializer=weightinit))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"softmax\"))  # Final classification layer\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=learning_rate),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model in JSON format and the model weights in HDF5 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "keras_model_weights = os.path.join(data_dir, \"060220_gridss_model.h5\")\n",
    "keras_model_json = os.path.join(data_dir, \"060220_gridss_model.json\")\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(keras_model_json, \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(keras_model_weights)\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepLift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow\n",
    "print(\"Tensorflow version:\", tensorflow.__version__)\n",
    "import keras\n",
    "print(\"Keras version:\", keras.__version__)\n",
    "import numpy\n",
    "print(\"Numpy version:\", numpy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -y -c bioconda deeplift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplift\n",
    "from keras.models import model_from_json\n",
    "\n",
    "#load the keras model\n",
    "#model files should be: model weights in HDF5 format and model in JSON format\n",
    "\n",
    "keras_model = model_from_json(open(keras_model_json).read())\n",
    "keras_model.load_weights(keras_model_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the Keras model for DeepLift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeplift.layers import NonlinearMxtsMode\n",
    "import deeplift.conversion.kerasapi_conversion as kc\n",
    "from collections import OrderedDict\n",
    "\n",
    "method_to_model = OrderedDict()\n",
    "for method_name, nonlinear_mxts_mode in [\n",
    "    #The genomics default = rescale on conv layers, revealcancel on fully-connected\n",
    "    ('rescale_conv_revealcancel_fc', NonlinearMxtsMode.DeepLIFT_GenomicsDefault),\n",
    "    ('rescale_all_layers', NonlinearMxtsMode.Rescale),\n",
    "    ('revealcancel_all_layers', NonlinearMxtsMode.RevealCancel),\n",
    "    ('grad_times_inp', NonlinearMxtsMode.Gradient),\n",
    "    ('guided_backprop', NonlinearMxtsMode.GuidedBackprop)]:\n",
    "    method_to_model[method_name] = kc.convert_model_from_saved_files(\n",
    "        h5_file=keras_model_weights,\n",
    "        json_file=keras_model_json,\n",
    "        nonlinear_mxts_mode=nonlinear_mxts_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the input data (windows and labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "truth_set = 'gridss'\n",
    "\n",
    "with np.load(os.path.join(data_dir, truth_set+'_windows.npz')) as data:\n",
    "    # print(type(data['start']))\n",
    "    input_data = data['start']\n",
    "    input_data_labels = ['DEL_start']*data['start'].shape[0]\n",
    "    #input_data = data['end']\n",
    "    #y.extend(['DEL_end']*data['end'].shape[0])\n",
    "\n",
    "with np.load(os.path.join(data_dir, 'negative_windows.npz')) as data:\n",
    "    input_data = np.concatenate([input_data, data['neg']], axis=0)\n",
    "    input_data_labels.extend(['noSV']*data['neg'].shape[0])\n",
    "\n",
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure predictions are the same as the original model\n",
    "from deeplift.util import compile_func\n",
    "model_to_test = method_to_model['rescale_conv_revealcancel_fc']\n",
    "deeplift_prediction_func = compile_func([model_to_test.get_layers()[0].get_activation_vars()],\n",
    "                                         model_to_test.get_layers()[-1].get_activation_vars())\n",
    "original_model_predictions = keras_model.predict(input_data, batch_size=200)\n",
    "converted_model_predictions = deeplift.util.run_function_in_batches(\n",
    "                                input_data_list=[input_data],\n",
    "                                func=deeplift_prediction_func,\n",
    "                                batch_size=200,\n",
    "                                progress_update=None)\n",
    "print(\"maximum difference in predictions:\",np.max(np.array(converted_model_predictions)-np.array(original_model_predictions)))\n",
    "assert np.max(np.array(converted_model_predictions)-np.array(original_model_predictions)) < 10**-5\n",
    "predictions = converted_model_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Compiling scoring functions\")\n",
    "method_to_scoring_func = OrderedDict()\n",
    "for method,model in method_to_model.items():\n",
    "    print(\"Compiling scoring function for: \"+method)\n",
    "    method_to_scoring_func[method] = model.get_target_contribs_func(find_scores_layer_idx=0,\n",
    "                                                                    target_layer_idx=-3)\n",
    "    \n",
    "#To get a function that just gives the gradients, we use the multipliers of the Gradient model\n",
    "gradient_func = method_to_model['grad_times_inp'].get_target_multipliers_func(find_scores_layer_idx=0,\n",
    "                                                                              target_layer_idx=-3)\n",
    "print(\"Compiling integrated gradients scoring functions\")\n",
    "integrated_gradients10_func = deeplift.util.get_integrated_gradients_function(\n",
    "    gradient_computation_function = gradient_func,\n",
    "    num_intervals=10)\n",
    "method_to_scoring_func['integrated_gradients10'] = integrated_gradients10_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot channels\n",
    "# print(input_data.shape)\n",
    "\n",
    "# for i, w in enumerate(input_data):\n",
    "#     print(i)\n",
    "#     plot_channels(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use mean as reference\n",
    "bg = np.mean(input_data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data[0,:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "method_to_task_to_scores = OrderedDict()\n",
    "for method_name, score_func in method_to_scoring_func.items():\n",
    "    print(\"on method\",method_name)\n",
    "    method_to_task_to_scores[method_name] = OrderedDict()\n",
    "    for task_idx in [0,1,2]:\n",
    "        scores = np.array(score_func(\n",
    "                    task_idx=task_idx,\n",
    "                    input_data_list=[X],\n",
    "                    input_references_list=[bg],\n",
    "                    batch_size=32,\n",
    "                    progress_update=None))\n",
    "        # print(scores.shape)\n",
    "        assert scores.shape == X.shape\n",
    "        #The sum over the ACGT axis in the code below is important! Recall that DeepLIFT\n",
    "        # assigns contributions based on difference-from-reference; if\n",
    "        # a position is [1,0,0,0] (i.e. 'A') in the actual sequence and [0.3, 0.2, 0.2, 0.3]\n",
    "        # in the reference, importance will be assigned to the difference (1-0.3)\n",
    "        # in the 'A' channel, (0-0.2) in the 'C' channel,\n",
    "        # (0-0.2) in the G channel, and (0-0.3) in the T channel. You want to take the importance\n",
    "        # on all four channels and sum them up, so that at visualization-time you can project the\n",
    "        # total importance over all four channels onto the base that is actually present (i.e. the 'A'). If you\n",
    "        # don't do this, your visualization will look very confusing as multiple bases will be highlighted at\n",
    "        # every position and you won't know which base is the one that is actually present in the sequence!\n",
    "        # scores = np.sum(scores, axis=2)\n",
    "        method_to_task_to_scores[method_name][task_idx] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save scores\n",
    "import numpy as np\n",
    "scores_file = os.path.join(data_dir, 'method_to_task_to_scores.npy')\n",
    "np.save(scores_file, method_to_task_to_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load scores\n",
    "import numpy as np\n",
    "\n",
    "method_to_task_to_scores = np.load(scores_file, allow_pickle=True).item()\n",
    "# method_to_task_to_scores_loaded\n",
    "print(method_to_task_to_scores.keys())\n",
    "for k in method_to_task_to_scores.keys():\n",
    "    for i in [0,1,2]:\n",
    "        print(method_to_task_to_scores[k][i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to visualize windows and scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib\n",
    "\n",
    "def plot_window(W, idx, method_name, task):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import minmax_scale\n",
    "    from matplotlib import colors\n",
    "    \n",
    "    ch_names = ['COV','CDF','CDR','MRQ','MBQ','SNV','LCF','LCR','RCF','RCR',\n",
    "           'DRF','DRR','DLF','DLR','LDF','LDR','RDF','RDR','IRF','IRR',\n",
    "           'LSF','LSR', 'RSF','RSR','INB','INA','DUB','DUA','TRO','TRS','FLD','FRD','FAD',\n",
    "           'RLD','RRD','RAD','SLF','SLR','SRF','SRR','MAP','_A_','_T_',\n",
    "            '_C_','_G_','_N_'\n",
    "           ]\n",
    "    \n",
    "    scores = method_to_task_to_scores[method_name][task]\n",
    "    df_mask = pd.DataFrame(scores[idx], columns=ch_names)\n",
    "\n",
    "    n_ch = len(ch_names)\n",
    "    W_i = minmax_scale(W[idx,:,:], feature_range=(0, 1), axis=0, copy=True )\n",
    "\n",
    "    df = pd.DataFrame(W_i, columns=ch_names)\n",
    "    # print(df)\n",
    "    ax = df.plot(subplots=True, figsize=(15, 10), kind='line',\n",
    "                 legend=False, color = 'black')\n",
    "    \n",
    "    for i, a in enumerate(ax):\n",
    "        #a.fill_between(df.index, df[ch_names[i]].min(), df[ch_names[i]], color=a.get_lines()[0].get_color())\n",
    "        if i != n_ch-1:\n",
    "            a.spines['bottom'].set_color('white')\n",
    "        if i != 0:\n",
    "            a.spines['top'].set_color('white') \n",
    "        a.set_yticks([])\n",
    "        a.set_ylabel(ch_names[i], rotation=0, va='center', ha='right')\n",
    "        #extent = [x[0]-(x[1]-x[0])/2., x[-1]+(x[1]-x[0])/2.,0,1]\n",
    "        vmin=df_mask.min().min()\n",
    "        vmax=df_mask.max().max()\n",
    "        a.imshow(df_mask[ch_names[i]][np.newaxis,:], cmap=\"bwr\", aspect=\"auto\", alpha=1,\n",
    "                norm=colors.Normalize(vmin=vmin, vmax=vmax))\n",
    "        a.set_ylim(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_name = 'rescale_all_layers'\n",
    "task = 2\n",
    "#print(df_mask)\n",
    "plot_window(X, 4000, method_name, task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
