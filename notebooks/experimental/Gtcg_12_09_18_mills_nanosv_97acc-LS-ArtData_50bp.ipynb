{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trains and  tests on parts of  NA12878 data for Del_st, Del_end and No_sv, ground  truth Mills_nanosv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dependences and setting output configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import keras\n",
    "import gzip\n",
    "from collections import Counter\n",
    "%pylab inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from npy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# dico = pickle.load(open(\"/hpc/cog_bioinf/ridder/users/lsantuari/Processed/Test/060818/TestData_060818/NA12878/MultiLabelData/labels.pickle\", \"rb\"))\n",
    "# labels = dataset = dico[\"Mills2011_nanosv\"]\n",
    "\n",
    "#dico = pickle.load(open(\"/hpc/cog_bioinf/ridder/users/lsantuari/Processed/Test/060818/TestData_060818/PATIENT1/MultiLabelData/labels.pickle\", \"rb\"))\n",
    "\n",
    "\n",
    "label_file = \"/hpc/cog_bioinf/ridder/users/lsantuari/Processed/Test/111018/50bp/TestData_111018/NA12878/MultiLabelData/labels.pickle.gz\"\n",
    "\n",
    "with gzip.GzipFile(label_file, \"rb\") as f:\n",
    "    dico = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "print(dico.keys())\n",
    "labels = dico[\"Mills2011_nanosv\"]\n",
    "\n",
    "window_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datapath = '/hpc/cog_bioinf/ridder/users/lsantuari/Processed/Test/060818/TestData_060818/NA12878' \n",
    "#datapath = '/hpc/cog_bioinf/ridder/users/lsantuari/Processed/Test/060818/TestData_060818/PATIENT1/'\n",
    "\n",
    "sample_name = 'NA12878'\n",
    "datapath = '/hpc/cog_bioinf/ridder/users/lsantuari/Processed/Test/111018/50bp/TestData_111018/'+sample_name+'/'\n",
    "\n",
    "germline_1_datapath = datapath + '/ChannelData/'\n",
    "#ger_1_labels_datapath = datapath + '/LabelData/'\n",
    "\n",
    "n_channels = 38\n",
    "\n",
    "G1_data_SV = np.empty([0, n_channels, window_size])\n",
    "G1_labels_SV =[]\n",
    "G1_data_noSV = np.empty([0, n_channels, window_size])\n",
    "G1_labels_noSV =[]\n",
    "\n",
    "for i in range(1,23,1):\n",
    "        print('Loading Chr'+str(i))\n",
    "        #data_file = germline_1_datapath + str(i) +'_channel_maker_real_germline.npy.gz'\n",
    "        \n",
    "        data_file = germline_1_datapath + sample_name + '_' + str(i) +'.npy.gz'\n",
    "        \n",
    "        with gzip.GzipFile(data_file, \"rb\") as f:\n",
    "            G1_data_new = np.load(f)\n",
    "        f.close()\n",
    "        \n",
    "        print(G1_data_new.shape)\n",
    "        print(Counter(labels[str(i)]))\n",
    "        #G1_labels_new = np.load(ger_1_labels_datapath  +  str(i) +'_label_ci_full_overlap.npy/' +str(i) +'_label_ci_full_overlap.npy') \n",
    "        G1_labels_new = np.array(labels[str(i)])\n",
    "        print(G1_labels_new[0])\n",
    "        G1_data_new = G1_data_new[np.where(G1_labels_new != 'UK')]\n",
    "        G1_labels_new = G1_labels_new[np.where(G1_labels_new != 'UK')]\n",
    "        G1_data_SV_new  = G1_data_new[np.where(G1_labels_new != 'noSV')]\n",
    "        G1_labels_SV_new  = G1_labels_new[np.where(G1_labels_new != 'noSV')]\n",
    "        n_windows_sv = G1_data_SV_new.shape[0]\n",
    "        G1_data_noSV_new  = G1_data_new[np.where(G1_labels_new == 'noSV')]\n",
    "        G1_labels_noSV_new  = G1_labels_new[np.where(G1_labels_new == 'noSV')]\n",
    "        G1_data_noSV_new =  G1_data_noSV_new[0:n_windows_sv]\n",
    "        G1_labels_noSV_new =  G1_labels_noSV_new[0:n_windows_sv]\n",
    "        print(str(i) + ':' + str(G1_data_SV_new.shape))\n",
    "        print(G1_labels_SV_new[0])\n",
    "        G1_data_SV = np.concatenate((G1_data_SV, G1_data_SV_new))\n",
    "        G1_labels_SV = np.concatenate((G1_labels_SV, G1_labels_SV_new))\n",
    "        G1_data_noSV = np.concatenate((G1_data_noSV, G1_data_noSV_new))\n",
    "        G1_labels_noSV = np.concatenate((G1_labels_noSV, G1_labels_noSV_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find  the indexes that split the categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_splitting_indices(labels):\n",
    "    for i in range (0, labels.shape[0] - 1):\n",
    "        if (labels[i] != labels[i+1]):\n",
    "            print(i , ': ', labels[i] , labels[i+1] )        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find_splitting_indices(G1_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#G1_labels_SV = ['DEL'] * G1_labels_SV.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv=G1_data_SV.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(sv/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G1_data_noSV  = G1_data_noSV[0:int(sv/2)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G1_labels_noSV  = G1_labels_noSV[0:int(sv/2)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SV_train = 1600\n",
    "noSV_train = 800\n",
    "SV_val = 350\n",
    "noSV_val = 175"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate((G1_data_SV[0:SV_train], G1_data_noSV[0:noSV_train ]))\n",
    "X_val = np.concatenate((G1_data_SV[SV_train:SV_train+SV_val], G1_data_noSV[noSV_train:noSV_train +noSV_val ]))\n",
    "X_test = np.concatenate((G1_data_SV[SV_train+SV_val:], G1_data_noSV[noSV_train +noSV_val:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.concatenate((G1_labels_SV[0:SV_train], G1_labels_noSV[0:noSV_train ]))\n",
    "y_val = np.concatenate((G1_labels_SV[SV_train:SV_train+SV_val], G1_labels_noSV[noSV_train:noSV_train +noSV_val ]))\n",
    "y_test = np.concatenate((G1_labels_SV[SV_train+SV_val:], G1_labels_noSV[noSV_train +noSV_val:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of training, validation and test windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_channels = X_train.shape[1]\n",
    "number_channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize data using the reads coverage as parameter and remove uninteresting channels for deletions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage = np.median(X_train[:, 0, :]) #the median of the coverage in the normal cells\n",
    "coverage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X):       \n",
    "    for i in range (0, len(X)):                              \n",
    "        for j in range(0,number_channels):                                                                                            \n",
    "            X[i][j] = np.divide(X[i][j], coverage)*100 \n",
    "# normalize(X_train)\n",
    "# normalize(X_val)\n",
    "# normalize(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminate_noise_chanels(X):\n",
    "    return np.delete(X, [4-1,5-1,6-1,7-1,8-1,9-1,10-1,12-1,13-1,15-1,16-1, 17-1,18-1,19-1,23-1,24-1,26-1,27-1], axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminate_distance_chanels(X):\n",
    "    return np.delete(X, [3,4,7,8], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = eliminate_noise_chanels(X_train)\n",
    "# X_val = eliminate_noise_chanels(X_val)\n",
    "# X_test = eliminate_noise_chanels(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = eliminate_distance_chanels(X_train)\n",
    "# X_val = eliminate_distance_chanels(X_val)\n",
    "# X_test = eliminate_distance_chanels(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_channels = X_train.shape[1]\n",
    "number_channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentiate the time series for a zero mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def differentiate(X):\n",
    "    for i in range(0, X.shape[0]):        \n",
    "        for j in range(0, X.shape[1]):\n",
    "            for k in range(0, X.shape[2]-1):\n",
    "                X[i][j][k] = X[i][j][k+1] - X[i][j][k]\n",
    "    return X[:,:,:-1]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = differentiate(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_val = differentiate(X_val)\n",
    "# X_test = differentiate(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots several  windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_windows = 100\n",
    "label = [\"None\"] * number_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label[0] = \"germline:coverage\"\n",
    "label[1] = \"germline:#left clipped reads\"\n",
    "label[2] = \"germline:#right clipped reads\"\n",
    "label[3] = \"germline:#split reads right split\"\n",
    "label[4] = \"germline:#split reads left split\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colors =['b','g', 'm', 'gray', 'r','c', 'darkgreen' ,'y', 'k', 'orange', 'gray']\n",
    "\n",
    "def plot_channels(start_window, n_windows):\n",
    "    for i in range (start_window, start_window + n_windows, 1):\n",
    "        if y_test[i] == labels[predicted[i]]:\n",
    "            continue\n",
    "        #print(y_train[i], 'id:', i)\n",
    "        print('id:', i)\n",
    "        print('Nanopore:', y_test[i], '. DeepSV:', labels[predicted[i]])\n",
    "        for j in range (0, number_channels):   \n",
    "            shift = 0\n",
    "            start =0\n",
    "            if j in [0,11]:\n",
    "                shift = -100\n",
    "            if j > 8:\n",
    "                start = -60\n",
    "            Z = [start + shift + 300 + x - 5*j*4 for x in X_test[i][j]]  \n",
    "            plt.ylim([150,450])\n",
    "            if np.max(Z) > 1000:\n",
    "                plt.ylim([-200,1000])\n",
    "                Z = [start + shift + 300 + x - 5*j*4 - 400 for x in X_test[i][j]]  \n",
    "            if j>8:\n",
    "                plt.plot(Z,  label = label[j], linestyle=':', linewidth=1.5, color = colors[j % 9])\n",
    "            else:\n",
    "                plt.plot(Z,  label = label[j], color = colors[j])\n",
    "        \n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., prop={'size': 10})    \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_window = 0\n",
    "n_windows = 573\n",
    "plot_channels(start_window, n_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_windows = 50\n",
    "start_window = 500\n",
    "plot_channels(start_window, n_windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "## Transposes every window in X, to comply to McFly format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transposeDataset(X):\n",
    "    image = []\n",
    "    for i in range (0, len(X -1)):\n",
    "        tr = X[i].transpose()\n",
    "        image.append(tr)\n",
    "    return np.array(image)\n",
    "\n",
    "image1 = transposeDataset(X_train)\n",
    "image2 = transposeDataset(X_val)\n",
    "image3 = transposeDataset(X_test)\n",
    "X_train = image1\n",
    "X_val = image2\n",
    "X_test = image3\n",
    "\n",
    "n_channels = len(X_train[0][0])\n",
    "print(len(X_test[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the labels in McFly format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapclasses = {'DEL_start': 1, 'DEL_end': 0,  'noSV': 2}\n",
    "print(mapclasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train = np.array([mapclasses[c] for c in y_train], dtype='int')\n",
    "y_val = np.array([mapclasses[c] for c in y_val], dtype='int')\n",
    "y_test = np.array([mapclasses[c] for c in y_test], dtype='int')\n",
    "y_train_binary = to_categorical(y_train)\n",
    "y_val_binary = to_categorical(y_val)\n",
    "y_test_binary = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Generate and train neural networks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Convolution1D, Flatten, MaxPooling1D\n",
    "from keras.optimizers import Adam\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "from mcfly import modelgen, find_architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed = 321\n",
    "num_classes = y_train_binary.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "models = modelgen.generate_models(X_train.shape,\n",
    "                                  num_classes,\n",
    "                                  number_of_models = 1,\n",
    "                                  model_type = 'CNN',\n",
    "                                  cnn_min_layers=1,\n",
    "                                  cnn_max_layers=1,\n",
    "                                  cnn_min_filters = 3, \n",
    "                                  cnn_max_filters = 3, \n",
    "                                  cnn_min_fc_nodes=2, \n",
    "                                  cnn_max_fc_nodes=2,\n",
    "                                  low_lr=1.73, high_lr=1.73,\n",
    "                                  kernel_size = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "models = modelgen.generate_models(X_train.shape,\n",
    "                                  num_classes,\n",
    "                                  number_of_models = 1,\n",
    "                                  model_type = 'CNN',\n",
    "                                  cnn_min_layers=2,\n",
    "                                  cnn_max_layers=2,\n",
    "                                  cnn_min_filters = 4, \n",
    "                                  cnn_max_filters = 4, \n",
    "                                  cnn_min_fc_nodes=6, \n",
    "                                  cnn_max_fc_nodes=6,\n",
    "                                  low_lr=2, high_lr=2)\n",
    "                                  #kernel_size = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "i=0\n",
    "for model, params, model_types in models:\n",
    "    print('model ' + str(i))\n",
    "    i=i+1\n",
    "    print(params)\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_size = X_train.shape[0]\n",
    "train_set_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "histories, val_accuracies, val_losses = find_architecture.train_models_on_samples(X_train, y_train_binary,\n",
    "                                                                                 X_val, y_val_binary,\n",
    "                                                                                 models,nr_epochs=1, \n",
    "                                                                                 subset_size=train_set_size,\n",
    "                                                                                 verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_index = np.argmax(val_accuracies)\n",
    "best_model, best_params, best_model_types = models[best_model_index]\n",
    "print(best_model_index, best_model_types, best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model_path = os.path.join('/hpc/cog_bioinf/ridder/users/lsantuari/Processed/Test/060818/TestData_060818/NA12878/ModelData', 'Gtcg_13_9')\n",
    "\n",
    "best_model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the best model on the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We make a copy of the model, to start training from fresh\n",
    "nr_epochs = 1\n",
    "datasize = train_set_size # Change in `X_train.shape[0]` if training complete data set\n",
    "history = best_model.fit(X_train[:datasize,:,:], y_train_binary[:datasize,:],\n",
    "              epochs=nr_epochs, validation_data=(X_val, y_val_binary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.models import load_model\n",
    "#model_path = os.path.join('/hpc/cog_bioinf/ridder/users/lsantuari/Processed/Test/060818/TestData_060818/NA12878/ModelData',  'Gtcg_13_9')\n",
    "\n",
    "model_path = '/hpc/cog_bioinf/ridder/users/lsantuari/Processed/Test/111018/50bp/TrainingData_111018/data/models_DEL/best_model'\n",
    "\n",
    "model_reloaded = load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect model predictions on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = model_reloaded \n",
    "datasize = X_test.shape[0]\n",
    "probs = model.predict_proba(X_test[:datasize,:,:],batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns are predicted labels by DeepSV, rows are nanopore labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapclasses = {'DEL_end': 0, 'DEL_start': 1, 'noSV': 2}\n",
    "dict=mapclasses\n",
    "dict_sorted = sorted(dict.items(), key=lambda x: x[1])\n",
    "dict_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [i[0] for i in dict_sorted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "predicted = probs.argmax(axis=1)\n",
    "y_index = y_test_binary.argmax(axis=1)\n",
    "confusion_matrix = pd.crosstab(pd.Series(y_index), pd.Series(predicted))\n",
    "confusion_matrix.index = [labels[i] for i in confusion_matrix.index]\n",
    "confusion_matrix.columns = [labels[i] for i in confusion_matrix.columns]\n",
    "confusion_matrix.reindex(columns=[l for l in labels], fill_value=0)\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test best current model on testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall accuracy on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "score_test = model.evaluate(X_test, y_test_binary, verbose=False)\n",
    "print('Test loss and accuracy of best model: ' + str(score_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conf = pd.DataFrame()\n",
    "\n",
    "for i in np.linspace(1.0 / len(labels), 1, num=50, endpoint=False):\n",
    "\n",
    "    predicted = np.argwhere(probs > i)[:, 1]\n",
    "    y_index = np.argwhere(y_test_binary > i)[:, 1]\n",
    "\n",
    "    # Rows: true, columns: predicted\n",
    "    confusion_matrix = pd.crosstab(pd.Series(y_index), pd.Series(predicted))\n",
    "    confusion_matrix.index = [labels[i] for i in confusion_matrix.index]\n",
    "    confusion_matrix.columns = [labels[i] for i in confusion_matrix.columns]\n",
    "    confusion_matrix.reindex(columns=[l for l in labels], fill_value=0)\n",
    "\n",
    "    #print('Confusion matrix:')\n",
    "    #print(confusion_matrix)\n",
    "    \n",
    "    for l in labels:\n",
    "        if l in confusion_matrix.index:\n",
    "            # print(confusion_matrix.loc[l,:])\n",
    "            # print(confusion_matrix.loc[:,l])\n",
    "\n",
    "            # label_correct = confusion_matrix.loc[l, l]\n",
    "            label_precision = np.around(confusion_matrix.loc[l, l] / sum(confusion_matrix.loc[:, l]) * 100)\n",
    "            label_recall = np.around(confusion_matrix.loc[l, l] / sum(confusion_matrix.loc[l, :]) * 100)\n",
    "            label_F1 = 2 * (label_precision * label_recall) / (label_precision + label_recall)\n",
    "\n",
    "            # print(f'Iter:{i} {l} -> Precision:{label_precision}%, Recall:{label_recall}%, F1:{label_F1}')\n",
    "\n",
    "            df_intres = pd.DataFrame(\n",
    "                {'iteration': [i], 'label': [l],\n",
    "                 'precision': [label_precision], 'recall': [label_recall], 'F1': [label_F1]})\n",
    "            df_conf = df_conf.append(df_intres)\n",
    "            \n",
    "print(df_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_conf[df_conf['label']=='DEL_start']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(df_conf[df_conf['label']=='DEL_start']['precision'])\n",
    "plt.ylabel('some numbers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
