{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trains and  tests on parts of  NA12878 data for Del_st, Del_end and No_sv, ground  truth Mills_nanosv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dependences and setting output configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import keras\n",
    "import gzip\n",
    "from collections import Counter\n",
    "\n",
    "%pylab inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from npy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['nanosv', 'nanosv_manta', 'manta', 'delly', 'lumpy', 'gridss', 'Mills2011', 'Mills2011_nanosv', 'nanosv_Mills2011', 'Mills2011_nanosv_manta', 'Mills2011_PacBio_Moleculo', 'Mills2011_PacBio_Moleculo_nanosv', 'nanosv_Mills2011_PacBio_Moleculo', 'Mills2011_PacBio_Moleculo_nanosv_manta', 'Mills2011_PacBio_Moleculo_Lumpy_GASVPro_DELLY_Pindel', 'id'])\n"
     ]
    }
   ],
   "source": [
    "# import pickle\n",
    "# dico = pickle.load(open(\"/hpc/cog_bioinf/ridder/users/lsantuari/Processed/Test/060818/TestData_060818/NA12878/MultiLabelData/labels.pickle\", \"rb\"))\n",
    "# labels = dataset = dico[\"Mills2011_nanosv\"]\n",
    "\n",
    "#sample_name = 'NA12878'\n",
    "\n",
    "sample_name = 'NA12878'\n",
    "\n",
    "#date = '201118'\n",
    "date = '231118'\n",
    "\n",
    "#dico = pickle.load(open(\"/hpc/cog_bioinf/ridder/users/lsantuari/Processed/Test/060818/TestData_060818/PATIENT1/MultiLabelData/labels.pickle\", \"rb\"))\n",
    "dico_file = '/hpc/cog_bioinf/ridder/users/lsantuari/Processed/Test/'+date+'/TestData_'+date+'/'+sample_name+'/MultiLabelData/labels.pickle.gz'\n",
    "\n",
    "with gzip.GzipFile(dico_file, \"rb\") as f:\n",
    "    dico = np.load(f)\n",
    "f.close()\n",
    "\n",
    "print(dico.keys())\n",
    "\n",
    "window_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', 'X']\n",
      "Loading data for Chr4\n",
      "Loading data for Chr5\n",
      "Loading data for Chr6\n",
      "Loading data for Chr7\n",
      "Loading data for Chr8\n",
      "Loading data for Chr9\n",
      "Loading data for Chr10\n",
      "Loading data for Chr11\n",
      "Loading data for Chr12\n",
      "Loading data for Chr13\n",
      "Loading data for Chr14\n",
      "Loading data for Chr15\n",
      "Loading data for Chr16\n",
      "Loading data for Chr17\n",
      "Loading data for Chr18\n",
      "Loading data for Chr19\n",
      "Loading data for Chr20\n",
      "Loading data for Chr21\n",
      "Loading data for Chr22\n",
      "Loading data for ChrX\n",
      "Counter({'noSV': 925494, 'UK': 1475, 'DEL_start': 1051, 'DEL_end': 949})\n"
     ]
    }
   ],
   "source": [
    "# Load channel data and labels by chromosome\n",
    "\n",
    "#Leaving out chromosome Y and MT for the moment\n",
    "chr_list = list(map(str, np.arange(4,23)))\n",
    "chr_list.append('X')\n",
    "\n",
    "#chr_list = ['22']\n",
    "print(chr_list)\n",
    "\n",
    "data_original = []\n",
    "labels_original = []\n",
    "#label_type = 'Mills2011_nanosv'\n",
    "label_type = 'Mills2011_nanosv'\n",
    "datapath = '/hpc/cog_bioinf/ridder/users/lsantuari/Processed/Test/'+date+'/TestData_'+date+'/'+sample_name+'/ChannelData/'\n",
    "\n",
    "for i in chr_list:\n",
    "#for i in [16, 17, 18]:\n",
    "    \n",
    "    print('Loading data for Chr%s' % i)\n",
    "    data_file = datapath + sample_name + '_' + str(i) + '.npy.gz'\n",
    "    with gzip.GzipFile(data_file, \"rb\") as f:\n",
    "        data_mat = np.load(f)\n",
    "        data_original.extend(data_mat)\n",
    "    f.close()\n",
    "    \n",
    "    labels_original.extend(dico[label_type][i])\n",
    "    \n",
    "print(Counter(labels_original))\n",
    "assert len(data_original) == len(labels_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'noSV': 81908, 'UK': 161, 'DEL_end': 116, 'DEL_start': 112})\n",
      "Counter({'noSV': 56835, 'DEL_start': 114, 'DEL_end': 100, 'UK': 100})\n",
      "Counter({'noSV': 61932, 'UK': 151, 'DEL_start': 87, 'DEL_end': 75})\n",
      "Counter({'noSV': 67146, 'UK': 92, 'DEL_start': 91, 'DEL_end': 72})\n",
      "Counter({'noSV': 49394, 'UK': 98, 'DEL_start': 76, 'DEL_end': 59})\n",
      "Counter({'noSV': 47574, 'DEL_end': 66, 'DEL_start': 61, 'UK': 49})\n",
      "Counter({'noSV': 72671, 'UK': 96, 'DEL_start': 57, 'DEL_end': 50})\n",
      "Counter({'noSV': 50178, 'UK': 86, 'DEL_end': 60, 'DEL_start': 59})\n",
      "Counter({'noSV': 51421, 'UK': 72, 'DEL_end': 58, 'DEL_start': 56})\n",
      "Counter({'noSV': 31937, 'DEL_start': 58, 'DEL_end': 48, 'UK': 42})\n",
      "Counter({'noSV': 31267, 'UK': 64, 'DEL_start': 38, 'DEL_end': 28})\n",
      "Counter({'noSV': 29985, 'DEL_start': 32, 'UK': 30, 'DEL_end': 29})\n",
      "Counter({'noSV': 56677, 'UK': 125, 'DEL_start': 27, 'DEL_end': 20})\n",
      "Counter({'noSV': 41098, 'UK': 68, 'DEL_end': 25, 'DEL_start': 23})\n",
      "Counter({'noSV': 33309, 'UK': 51, 'DEL_start': 40, 'DEL_end': 37})\n",
      "Counter({'noSV': 40619, 'UK': 43, 'DEL_start': 25, 'DEL_end': 19})\n",
      "Counter({'noSV': 28012, 'UK': 48, 'DEL_start': 29, 'DEL_end': 29})\n",
      "Counter({'noSV': 28414, 'UK': 17, 'DEL_start': 14, 'DEL_end': 14})\n",
      "Counter({'noSV': 17332, 'UK': 41, 'DEL_start': 14, 'DEL_end': 7})\n",
      "Counter({'noSV': 47785, 'UK': 41, 'DEL_start': 38, 'DEL_end': 37})\n"
     ]
    }
   ],
   "source": [
    "for i in chr_list:\n",
    "    print(Counter(dico[label_type][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'noSV': 925494, 'UK': 1475, 'DEL_start': 1051, 'DEL_end': 949})\n"
     ]
    }
   ],
   "source": [
    "labels_original = []\n",
    "label_type = 'Mills2011_nanosv'\n",
    "#label_type = 'nanosv_manta'\n",
    "\n",
    "for i in chr_list:\n",
    "    #print('Loading data for Chr%s' % i)\n",
    "    labels_original.extend(dico[label_type][i])\n",
    "    \n",
    "print(Counter(labels_original))\n",
    "\n",
    "id_original = []\n",
    "for i in chr_list:\n",
    "    #print('Loading data for Chr%s' % i)\n",
    "    id_original.extend(dico['id'][i])\n",
    "id_original = [str(v['chromosome'])+':'+str(v['position']) for v in id_original]\n",
    "# print(id_original)\n",
    "\n",
    "labels_original = np.array(labels_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({True: 898125, False: 30844})\n",
      "Counter({'noSV': 916473, 'UK': 5380, 'DEL_start': 3564, 'DEL_end': 3552})\n",
      "Counter({'noSV': 894285, 'UK': 32516, 'DEL_start': 1111, 'DEL_end': 1057})\n"
     ]
    }
   ],
   "source": [
    "labels_svcaller = {}\n",
    "#label_type = 'nanosv_Mills2011'\n",
    "\n",
    "for label_type in ['manta', 'gridss', 'lumpy', 'delly']:\n",
    "    labels_svcaller[label_type] = []\n",
    "    for chr in chr_list:\n",
    "        labels_svcaller[label_type].extend(dico[label_type][chr])\n",
    "    labels_svcaller[label_type] = np.array(labels_svcaller[label_type])\n",
    "\n",
    "manta_gridss = (labels_svcaller['manta']==labels_svcaller['gridss'])\n",
    "lumpy_delly = (labels_svcaller['lumpy']==labels_svcaller['delly'])\n",
    "#manta_nanosv = (labels_svcaller['manta']==labels_svcaller['nanosv'])\n",
    "manta_gridss_lumpy_delly = (manta_gridss == lumpy_delly)\n",
    "#manta_gridss_lumpy_delly_nanosv = (manta_gridss_lumpy_delly == manta_nanosv)\n",
    "\n",
    "print(Counter(manta_gridss_lumpy_delly))\n",
    "\n",
    "labels = []\n",
    "for i in np.arange(len(labels_svcaller['manta'])):\n",
    "    if manta_gridss_lumpy_delly[i]:\n",
    "            labels.append(labels_svcaller['manta'][i])\n",
    "    else:\n",
    "        labels.append('UK')\n",
    "\n",
    "print(Counter(labels_svcaller['manta']))\n",
    "print(Counter(labels))\n",
    "\n",
    "assert len(labels_svcaller['manta']) == len(labels)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(928969, 44, 200)\n",
      "928969\n"
     ]
    }
   ],
   "source": [
    "data_original = np.array(data_original)\n",
    "\n",
    "print(data_original.shape)\n",
    "print(len(labels_original))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_windows = 30\n",
    "number_channels = data_original.shape[1]\n",
    "label = [\"None\"] * number_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:coverage\n",
      "1:#left clipped reads\n",
      "2:#right clipped reads\n",
      "3:INV_before\n",
      "4:INV_after\n",
      "5:DUP_before\n",
      "6:DUP_after\n",
      "7:TRA_opposite\n",
      "8:TRA_same\n",
      "9:Forward_Left_Clipped_sum\n",
      "10:Forward_Left_Clipped_num\n",
      "11:Forward_Left_Clipped_median\n",
      "12:Forward_Left_Clipped_outliers\n",
      "13:Forward_Right_Clipped_sum\n",
      "14:Forward_Right_Clipped_num\n",
      "15:Forward_Right_Clipped_median\n",
      "16:Forward_Right_Clipped_outliers\n",
      "17:Forward_Not_Clipped_sum\n",
      "18:Forward_Not_Clipped_num\n",
      "19:Forward_Not_Clipped_median\n",
      "20:Forward_Not_Clipped_outliers\n",
      "21:Reverse_Left_Clipped_sum\n",
      "22:Reverse_Left_Clipped_num\n",
      "23:Reverse_Left_Clipped_median\n",
      "24:Reverse_Left_Clipped_outliers\n",
      "25:Reverse_Right_Clipped_sum\n",
      "26:Reverse_Right_Clipped_num\n",
      "27:Reverse_Right_Clipped_median\n",
      "28:Reverse_Right_Clipped_outliers\n",
      "29:Reverse_Not_Clipped_sum\n",
      "30:Reverse_Not_Clipped_num\n",
      "31:Reverse_Not_Clipped_median\n",
      "32:Reverse_Not_Clipped_outliers\n",
      "33:#left split reads\n",
      "34:#right split reads\n",
      "35:L_SplitRead_sum\n",
      "36:L_SplitRead_num\n",
      "37:L_SplitRead_median\n",
      "38:R_SplitRead_sum\n",
      "39:R_SplitRead_num\n",
      "40:R_SplitRead_median\n",
      "41:GC\n",
      "42:Mappability\n",
      "43:One_hot_Ncoding\n"
     ]
    }
   ],
   "source": [
    "# Fill labels for legend\n",
    "\n",
    "label[0] = \"coverage\"\n",
    "label[1] = \"#left clipped reads\"\n",
    "label[2] = \"#right clipped reads\"\n",
    "label[3] = \"INV_before\"\n",
    "label[4] = \"INV_after\"\n",
    "label[5] = \"DUP_before\"\n",
    "label[6] = \"DUP_after\"\n",
    "label[7] = \"TRA_opposite\"\n",
    "label[8] = \"TRA_same\"\n",
    "\n",
    "i = 9\n",
    "for direction in ['Forward','Reverse']:\n",
    "    for clipped in ['Left','Right','Not']:\n",
    "        for value in ['sum', 'num', 'median', 'outliers']:\n",
    "            label[i] = direction+'_'+clipped+'_Clipped_'+value\n",
    "            i = i+1\n",
    "\n",
    "label[i] = \"#left split reads\"\n",
    "i = i+1\n",
    "label[i] = \"#right split reads\"\n",
    "i = i+1\n",
    "\n",
    "for clipped in ['L','R']:\n",
    "    for value in ['sum', 'num', 'median']:\n",
    "        label[i] = clipped+'_SplitRead_'+value\n",
    "        i = i+1\n",
    "        \n",
    "label[i] = \"GC\"\n",
    "i = i+1\n",
    "label[i] = \"Mappability\"\n",
    "i = i+1\n",
    "label[i] = \"One_hot_Ncoding\"\n",
    "\n",
    "for k,l in enumerate(label):\n",
    "    print(str(k)+':'+l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the windows per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: '/hpc/cog_bioinf/ridder/users/lsantuari/Processed/Test/231118/TestData_231118/NA12878/ChannelData/../Figures'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-9f1593030c7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdatapath_fig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatapath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'../Figures'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatapath_fig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '/hpc/cog_bioinf/ridder/users/lsantuari/Processed/Test/231118/TestData_231118/NA12878/ChannelData/../Figures'"
     ]
    }
   ],
   "source": [
    "datapath_fig = datapath+'../Figures'\n",
    "os.mkdir(datapath_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_channels(indices, n_windows, output_path):\n",
    "    #for i in range (start_window, start_window + n_windows):\n",
    "    for idx in np.arange(n_windows):\n",
    "        \n",
    "        i = indices[idx]\n",
    "        #print(y_train[i], 'id:', i)\n",
    "        fig_name = labels_original[i]+'_'+'Chr'+id_original[i].replace(':','-')\n",
    "        \n",
    "        print(labels_original[i], 'id:', 'Chr'+id_original[i])\n",
    "        plt.title('Class: '+labels_original[i]+' '+'Position: Chr'+id_original[i], fontsize=30)\n",
    "        plt.ylim([0, number_channels])\n",
    "        plt.yticks(np.arange(number_channels), label, fontsize=15)\n",
    "        plt.xticks(fontsize=30)\n",
    "        \n",
    "        plt.vlines(x=0, ymin=0, ymax=number_channels, color='black')\n",
    "        \n",
    "        #fig, axs = plt.subplots(number_channels, 1)\n",
    "        \n",
    "        #for j in range (number_channels-1, -1, -1):\n",
    "        #for j in range (number_channels-1, -1, -1):\n",
    "        for j in range (number_channels-1, -1, -1):\n",
    "        #for j in [12,16,20,24,28,32]:\n",
    "            mul = 1\n",
    "            shift = 0\n",
    "            start = 0\n",
    "            #if j in [0,1,4,5]:\n",
    "                #shift = -60\n",
    "            #if j in [4,5,6,7]:\n",
    "                #start = 70\n",
    "            #print(X_train[i][j])\n",
    "            #X_win = data_original[i][j]\n",
    "            if sum(data_original[i][j])!=0:\n",
    "                X_win = ((data_original[i][j]-min(data_original[i][j]))/max(data_original[i][j]))\n",
    "                #X_win = data_original[i][j]\n",
    "            else:\n",
    "                X_win = data_original[i][j]\n",
    "            #Z = [start + shift + x + 5*j*4 for x in X_train[i][j]] \n",
    "            Z = [x + j for x in X_win] \n",
    "            \n",
    "            plt.plot(np.arange(-100,100), Z, label = label[j], linewidth=2)\n",
    "            plt.fill_between(np.arange(-100,100), j, Z, alpha=.5)\n",
    "            \n",
    "            #axs[j].plot(Z, label = label[j], linewidth=2)\n",
    "            #plt.setp(axs[j].get_xticklabels(), visible=False)\n",
    "            \n",
    "            #plt.fill_between(np.arange(len(Z)), 1, Z, alpha=1)\n",
    "            \n",
    "            #plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., prop={'size': 10})\n",
    "        #fig.tight_layout()\n",
    "        #plt.show()\n",
    "        plt.savefig(os.path.join(output_path, fig_name+'.png'))\n",
    "        plt.clf()\n",
    "        #\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svtype = 'DEL'\n",
    "idx = list(np.where(labels==svtype+\"_end\")[0])\n",
    "i = idx[0]\n",
    "j = 0\n",
    "X_win = ((data_original[i][j]-min(data_original[i][j]))/max(data_original[i][j]))\n",
    "#print(data_original[i][j])\n",
    "#print(X_win+1)\n",
    "print('min:%d, max:%d' % (X_win.min(), X_win.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#idx = np.where(labels==\"noSV\")\n",
    "#idx = np.where(labels==\"noSV\")\n",
    "svtype = 'DEL'\n",
    "#idx = np.where(labels_original==svtype+\"_start\")[0]\n",
    "class_label = svtype+\"_start\"\n",
    "\n",
    "for class_label in ['DEL_start', 'DEL_end', 'noSV']:\n",
    "    \n",
    "    idx = list(np.where(labels_original==class_label)[0])\n",
    "    #print(idx)\n",
    "    #idx = np.where(labels==svtype+\"_end\")\n",
    "\n",
    "    #print(idx)\n",
    "\n",
    "    datapath_fig_class = os.path.join(datapath_fig, class_label)\n",
    "    if not os.path.exists(datapath_fig_class):\n",
    "        os.mkdir(datapath_fig_class)\n",
    "\n",
    "    # start_window = 3990\n",
    "    plt.rcParams['figure.figsize'] = (30,20)\n",
    "    plot_channels(idx, 30, datapath_fig_class)\n",
    "    #print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ii, ch_name in enumerate(label):\n",
    "#     print('%d => %s'%(ii, ch_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "timestr = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "datapath_now = datapath+'../ModelData/'+timestr\n",
    "os.mkdir(datapath_now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove windows flagged as 'UK' (unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del data\n",
    "keep = np.where(np.array(labels_original)!='UK')\n",
    "#keep = np.where(np.array(labels)!='UK')\n",
    "\n",
    "data = data_original[keep]\n",
    "labels = labels_original[keep]\n",
    "#labels = labels[keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.arange(0, 9)\n",
    "# # outliers channels\n",
    "#idx = np.append(idx, [12,16,20,24,28,32], 0)\n",
    "# # split reads\n",
    "idx = np.append(idx, [33,34], 0)\n",
    "#idx = np.append(idx, [42,43], 0)\n",
    "print(idx)\n",
    "\n",
    "data = data[:,idx,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(datapath_now+'/Channels.txt','w') \n",
    "\n",
    "file.write('Using the following channels:\\n\\n')\n",
    "for k,lab_i in enumerate(idx):\n",
    "    file.write(str(k)+':'+label[lab_i]+'\\n')\n",
    "\n",
    "file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_lab = Counter(labels)\n",
    "min_v = min([v for k, v in cnt_lab.items()])\n",
    "max_v = max([v for k, v in cnt_lab.items()])\n",
    "\n",
    "print(cnt_lab)\n",
    "print('Minimum number of labels = ' + str(min_v))\n",
    "print('Maximum number of labels = ' + str(max_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_balanced = []\n",
    "labels_balanced = []\n",
    "\n",
    "#class_size = 10**5\n",
    "\n",
    "for l in cnt_lab.keys():\n",
    "    #print(l)\n",
    "    iw = np.where(labels==l)\n",
    "    #print(iw[0])\n",
    "    #ii = np.random.choice(a=iw[0], size=min_v, replace=False)\n",
    "    ii = iw[0][:min_v]\n",
    "    #ii = np.random.choice(a=iw[0], size=class_size, replace=True)\n",
    "    #print(ii)\n",
    "    data_balanced.extend(data[ii])\n",
    "    labels_balanced.extend(labels[ii])\n",
    "\n",
    "print(Counter(labels_balanced))\n",
    "\n",
    "X = np.array(data_balanced)\n",
    "y = np.array(labels_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = numpy.unique(np.where(np.isnan(X))[0])\n",
    "X = np.delete(X, idx, 0)\n",
    "y = np.delete(y, idx, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_channels = X.shape[1]\n",
    "number_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into training, validation and test set 60/20/20\n",
    "\n",
    "cnt_lab = Counter(y)\n",
    "n_lab = [v for v in cnt_lab.values()][0]\n",
    "i_train = int(n_lab*0.6)\n",
    "i_val = i_train + int(n_lab*0.2)\n",
    "print(i_train)\n",
    "print(i_val)\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_val = []\n",
    "y_val = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for l in cnt_lab.keys():\n",
    "    iw = np.where(y==l)\n",
    "    #print(iw[0])\n",
    "    #print(iw[0][:i_train])\n",
    "    X_train.extend(X[iw[0][:i_train]])\n",
    "    X_val.extend(X[iw[0][i_train:i_val]])\n",
    "    X_test.extend(X[iw[0][i_val:]])\n",
    "\n",
    "    y_train.extend(y[iw[0][:i_train]])\n",
    "    y_val.extend(y[iw[0][i_train:i_val]])\n",
    "    y_test.extend(y[iw[0][i_val:]])\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_val = np.array(X_val)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X.shape[0] == y.shape[0]\n",
    "print(int(X.shape[0]/10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def split_dataset(X, y, cv_fold):\n",
    "    \n",
    "    cnt_lab = Counter(y)\n",
    "    print(cnt_lab)\n",
    "    n_lab = [v for v in cnt_lab.values()][0]\n",
    "\n",
    "    fold_count = int(n_lab/cv_fold)\n",
    "    \n",
    "    res_X = defaultdict(dict)\n",
    "    res_y = defaultdict(dict)\n",
    "    \n",
    "    X_train_val = []\n",
    "    \n",
    "    for l in cnt_lab.keys():\n",
    "        iw = np.where(y==l)\n",
    "        for i in np.arange(cv_fold):\n",
    "            X_train_val.extend(X[iw[0][i*fold_count:(i+1)*fold_count]])\n",
    "            \n",
    "            \n",
    "    \n",
    "split_dataset(X, y, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "## Transposes every window in X, to comply to McFly format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transposeDataset(X):\n",
    "    image = []\n",
    "    for i in range (0, len(X -1)):\n",
    "        tr = X[i].transpose()\n",
    "        image.append(tr)\n",
    "    return np.array(image)\n",
    "\n",
    "image1 = transposeDataset(X_train)\n",
    "image2 = transposeDataset(X_val)\n",
    "image3 = transposeDataset(X_test)\n",
    "X_train = image1\n",
    "X_val = image2\n",
    "X_test = image3\n",
    "\n",
    "n_channels = len(X_train[0][0])\n",
    "print(len(X_test[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the labels in McFly format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapclasses = {'DEL_start': 1, 'DEL_end': 0,  'noSV': 2}\n",
    "print(mapclasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train = np.array([mapclasses[c] for c in y_train], dtype='int')\n",
    "y_val = np.array([mapclasses[c] for c in y_val], dtype='int')\n",
    "y_test = np.array([mapclasses[c] for c in y_test], dtype='int')\n",
    "y_train_binary = to_categorical(y_train)\n",
    "y_val_binary = to_categorical(y_val)\n",
    "y_test_binary = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Generate and train neural networks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Convolution1D, Flatten, MaxPooling1D\n",
    "from keras.optimizers import Adam\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "from mcfly import modelgen, find_architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "lines = inspect.getsource(modelgen)\n",
    "# print(lines)\n",
    "sfile = inspect.getsourcefile(modelgen)\n",
    "# print(sfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed = 321\n",
    "num_classes = y_train_binary.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "models = modelgen.generate_models(X_train.shape,\n",
    "                                  num_classes,\n",
    "                                  number_of_models = 10,\n",
    "                                  model_type = 'CNN',\n",
    "                                  cnn_min_layers=2,\n",
    "                                  cnn_max_layers=6,\n",
    "                                  cnn_min_filters = 4, \n",
    "                                  cnn_max_filters = 8, \n",
    "                                  cnn_min_fc_nodes=6, \n",
    "                                  cnn_max_fc_nodes=12,\n",
    "                                  low_lr=2, high_lr=4,\n",
    "                                  kernel_size = 7)\n",
    "\n",
    "# models = modelgen.generate_models(X_train.shape,\n",
    "#                                   num_classes,\n",
    "#                                   number_of_models = 1,\n",
    "#                                   model_type = 'CNN',\n",
    "#                                   cnn_min_layers=2,\n",
    "#                                   cnn_max_layers=2,\n",
    "#                                   cnn_min_filters = 4, \n",
    "#                                   cnn_max_filters = 4, \n",
    "#                                   cnn_min_fc_nodes=6, \n",
    "#                                   cnn_max_fc_nodes=6,\n",
    "#                                   low_lr=2, high_lr=2,\n",
    "#                                   kernel_size = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "i=0\n",
    "for model, params, model_types in models:\n",
    "    print('model ' + str(i))\n",
    "    i=i+1\n",
    "    print(params)\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_size = X_train.shape[0]\n",
    "train_set_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in np.arange(X_train.shape[2]):\n",
    "#     print(X_train[4597,:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert not np.any(np.isnan(X_train))\n",
    "#print(numpy.unique(np.where(np.isnan(X_train))[0]))\n",
    "#print(X_train[101])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "histories, val_accuracies, val_losses = find_architecture.train_models_on_samples(X_train, y_train_binary,\n",
    "                                                                                 X_val, y_val_binary,\n",
    "                                                                                 models, nr_epochs=1, \n",
    "                                                                                 subset_size=train_set_size,\n",
    "                                                                                 verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_index = np.argmax(val_accuracies)\n",
    "best_model, best_params, best_model_types = models[best_model_index]\n",
    "print(best_model_index, best_model_types, best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model_path = os.path.join(datapath_now, 'model')\n",
    "\n",
    "best_model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the best model on the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We make a copy of the model, to start training from fresh\n",
    "nr_epochs = 1\n",
    "datasize = train_set_size # Change in `X_train.shape[0]` if training complete data set\n",
    "history = best_model.fit(X_train, y_train_binary,\n",
    "              epochs=nr_epochs, validation_data=(X_val, y_val_binary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from keras.models import load_model\n",
    "# model_path = os.path.join(datapath+'../ModelData',  'Gtcg_13_9')\n",
    "# model_reloaded = load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect model predictions on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save(model_path)\n",
    "\n",
    "model = best_model \n",
    "datasize = X_test.shape[0]\n",
    "probs = model.predict_proba(X_test[:datasize,:,:],batch_size=1000, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns are predicted labels by DeepSV, rows are nanopore labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapclasses = {'DEL_end': 0, 'DEL_start': 1, 'noSV': 2}\n",
    "dict=mapclasses\n",
    "dict_sorted = sorted(dict.items(), key=lambda x: x[1])\n",
    "dict_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [i[0] for i in dict_sorted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "predicted = probs.argmax(axis=1)\n",
    "y_index = y_test_binary.argmax(axis=1)\n",
    "confusion_matrix = pd.crosstab(pd.Series(y_index), pd.Series(predicted))\n",
    "confusion_matrix.index = [labels[i] for i in confusion_matrix.index]\n",
    "confusion_matrix.columns = [labels[i] for i in confusion_matrix.columns]\n",
    "confusion_matrix.reindex(columns=[l for l in labels], fill_value=0)\n",
    "print(confusion_matrix)\n",
    "confusion_matrix.to_csv(path_or_buf=os.path.join(datapath_now,'model_confusion_matrix.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test best current model on testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall accuracy on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "score_test = model.evaluate(X_test, y_test_binary, verbose=False)\n",
    "print('Test loss and accuracy of best model: ' + str(score_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conf = pd.DataFrame()\n",
    "\n",
    "for i in np.linspace(1.0 / len(labels), 1, num=50, endpoint=False):\n",
    "\n",
    "    predicted = np.argwhere(probs > i)[:, 1]\n",
    "    y_index = np.argwhere(y_test_binary > i)[:, 1]\n",
    "\n",
    "    # Rows: true, columns: predicted\n",
    "    confusion_matrix = pd.crosstab(pd.Series(y_index), pd.Series(predicted))\n",
    "    confusion_matrix.index = [labels[i] for i in confusion_matrix.index]\n",
    "    confusion_matrix.columns = [labels[i] for i in confusion_matrix.columns]\n",
    "    confusion_matrix.reindex(columns=[l for l in labels], fill_value=0)\n",
    "\n",
    "    #print('Confusion matrix:')\n",
    "    #print(confusion_matrix)\n",
    "    \n",
    "    for l in labels:\n",
    "        if l in confusion_matrix.index:\n",
    "            # print(confusion_matrix.loc[l,:])\n",
    "            # print(confusion_matrix.loc[:,l])\n",
    "\n",
    "            # label_correct = confusion_matrix.loc[l, l]\n",
    "            label_precision = np.around(confusion_matrix.loc[l, l] / sum(confusion_matrix.loc[:, l]) * 100)\n",
    "            label_recall = np.around(confusion_matrix.loc[l, l] / sum(confusion_matrix.loc[l, :]) * 100)\n",
    "            label_F1 = 2 * (label_precision * label_recall) / (label_precision + label_recall)\n",
    "\n",
    "            # print(f'Iter:{i} {l} -> Precision:{label_precision}%, Recall:{label_recall}%, F1:{label_F1}')\n",
    "\n",
    "            df_intres = pd.DataFrame(\n",
    "                {'iteration': [i], 'label': [l],\n",
    "                 'precision': [label_precision], 'recall': [label_recall], 'F1': [label_F1]})\n",
    "            df_conf = df_conf.append(df_intres)\n",
    "            \n",
    "# print(df_conf)\n",
    "df_conf.to_csv(path_or_buf=os.path.join(datapath_now,'model_PrecRec.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_conf[df_conf['label']=='DEL_start']\n",
    "\n",
    "df_conf = pd.read_csv(filepath_or_buffer=os.path.join(datapath_now,'model_PrecRec.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(15)\n",
    "idx = 1\n",
    "for lab in labels:\n",
    "    ax = plt.subplot(1,3,idx)\n",
    "    for measure in ['precision', 'recall', 'F1']:\n",
    "        ax.plot(df_conf[df_conf['label']==lab]['iteration'],\n",
    "                 df_conf[df_conf['label']==lab][measure])\n",
    "    ax.set_title('Label: '+lab)\n",
    "    ax.set_ylabel('')\n",
    "    ax.set_xlabel('probability cutoff')\n",
    "    ax.legend()\n",
    "    idx += 1\n",
    "\n",
    "fig.savefig(os.path.join(datapath_now,'model_PrecRec.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating predictions for PATIENT1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "datapath_now = os.path.join(datapath, '../../NA12878/ModelData/20181126-104011')\n",
    "model_path = os.path.join(datapath_now,  'model')\n",
    "model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load channel data and labels by chromosome\n",
    "\n",
    "sample_name = 'PATIENT1'\n",
    "date = '231118'\n",
    "\n",
    "#Leaving out chromosome Y and MT for the moment\n",
    "chr_list = list(map(str, np.arange(1,23)))\n",
    "chr_list.append('X')\n",
    "#chr_list = ['22']\n",
    "print(chr_list)\n",
    "\n",
    "data_original = []\n",
    "datapath = '/hpc/cog_bioinf/ridder/users/lsantuari/Processed/Test/'+date+'/TestData_'+date+'/'+sample_name+'/ChannelData/'\n",
    "\n",
    "for i in chr_list:\n",
    "#for i in [16, 17, 18]:\n",
    "    \n",
    "    print('Loading data for Chr%s' % i)\n",
    "    data_file = datapath + sample_name + '_' + str(i) + '.npy.gz'\n",
    "    with gzip.GzipFile(data_file, \"rb\") as f:\n",
    "        data_mat = np.load(f)\n",
    "        data_original.extend(data_mat)\n",
    "    f.close()\n",
    "\n",
    "data_original = np.array(data_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_original\n",
    "\n",
    "idx = np.arange(0, 9)\n",
    "# # outliers channels\n",
    "idx = np.append(idx, [12,16,20,24,28,32], 0)\n",
    "# # split reads\n",
    "idx = np.append(idx, [33,34], 0)\n",
    "#idx = np.append(idx, [42,43], 0)\n",
    "print(idx)\n",
    "\n",
    "data = data[:,idx,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(data)\n",
    "y = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = numpy.unique(np.where(np.isnan(X))[0])\n",
    "X = np.delete(X, idx, 0)\n",
    "y = np.delete(y, idx, 0)\n",
    "id_label = np.delete(np.array(id_original), idx, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = transposeDataset(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Inspect model predictions on test data\n",
    "datasize = X.shape[0]\n",
    "probs = model.predict_proba(X, batch_size=1000, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapclasses = {'DEL_end': 0, 'DEL_start': 1, 'noSV': 2}\n",
    "dict=mapclasses\n",
    "dict_sorted = sorted(dict.items(), key=lambda x: x[1])\n",
    "dict_sorted\n",
    "labels = [i[0] for i in dict_sorted]\n",
    "\n",
    "np.save(file=os.path.join(datapath_now,sample_name+'_probs.npy'), arr=probs)\n",
    "predicted = probs.argmax(axis=1)\n",
    "y_pred = [labels[i] for i in predicted]\n",
    "print(Counter(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write results in BED file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(os.path.join(datapath_now,sample_name+'_predictions.bed'),'w') \n",
    "\n",
    "for k,lab_i in enumerate(y_pred):\n",
    "    coord = id_label[k].split(':')\n",
    "    file.write(str(coord[0])+'\\t'+str(int(coord[1])-1)\\\n",
    "               +'\\t'+str(coord[1])+'\\t'+lab_i+'\\n')\n",
    "\n",
    "file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
